{"cells":[{"cell_type":"markdown","metadata":{},"source":["### Chuanyang Jin, Alex Yan"]},{"cell_type":"markdown","metadata":{},"source":["The code below is for practicing different methods, which includes:\n","Preprocessing (filtering out meaningless messages and stop words);\n","3 ways of data transformation (Token Count, TF-IDF, BERT);\n","6 ways of training (Logistic Regression, SVM, Neural Networks, Naive Bayes, Random Forest, Gradient Boosting) with different regularizations, kernels, structures, etc.;\n","and an ensemble model."]},{"cell_type":"markdown","metadata":{},"source":["## File Loading"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","pd.options.mode.chained_assignment = None  # removing annoying warnings\n","import re\n","import seaborn as sns   \n","import matplotlib.pyplot as plt\n","\n","import warnings\n","warnings.filterwarnings('ignore')  # disable warnings\n","\n","from sklearn import feature_extraction\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import precision_recall_fscore_support\n","from sentence_transformers import SentenceTransformer\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.svm import SVC\n","from sklearn.neural_network import MLPClassifier\n","from sklearn.metrics import confusion_matrix\n","\n","\n","# Reading in the dataset\n","train_df = pd.read_csv(\"train.csv\")"]},{"cell_type":"markdown","metadata":{},"source":["## Preprocessing"]},{"cell_type":"markdown","metadata":{},"source":["We first clean up the meaningless words from the samples to reduce feature number & prevent overfitting."]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["# Clean up the text data\n","def clean_text(texts):\n","    # convert into lower letters to filter out repetition in features\n","    texts[:] = [text.lower() for text in texts]          \n","\n","    # http:// ... / word  or  http:// ... / word                 \n","    texts[:] = [re.sub(r'https?:\\/\\/.*\\/\\w*', 'URL', text) for text in texts]   \n","\n","    # meaningless things like mentioning a friend (e.g. @username)\n","    texts[:] = [re.sub(r'@\\w+([-.]\\w+)*', '', text) for text in texts]          \n","\n","    # meaningless symbols followed by &, such as '&amp', which is a meaningless web \n","    texts[:] = [re.sub(r'&\\w+([-.]\\w+)*', '', text) for text in texts]\n","\n","\n","\n","clean_text(train_df['text'])\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["## Feature Transformation 1\n","Use sklearn's built in feature extraction to create a sparse martrix where every word appeared in the dataset becomes a feature."]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["# Using sklearn.feature_extraction.text.CountVectorizer()\n","# This converts a collection of text documents to a matrix of token counts,\n","# and produces a sparse representation of the counts using scipy.sparse.csr_matrix.\n","\n","count_vectorizer = feature_extraction.text.CountVectorizer(stop_words='english')\n","X_trainval = count_vectorizer.fit_transform(train_df[\"text\"])\n","feature_transform = 'Token Count'"]},{"cell_type":"markdown","metadata":{},"source":["## Feature Transformation 2\n","Use another sklearn feature extraction method to apply tf-idf transformation to the feature vector."]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["# Using sklearn.feature_extraction.text.TfidfVectorizer()\n","# This converts a collection of text documents to a matrix of token counts, but adds tf-idf\n","# and produces a sparse representation of the counts using scipy.sparse.csr_matrix.\n","\n","tfidf_vectorizer = feature_extraction.text.TfidfVectorizer(stop_words='english')\n","X_trainval = tfidf_vectorizer.fit_transform(train_df[\"text\"])\n","feature_transform = 'TF-IDF'"]},{"cell_type":"markdown","metadata":{},"source":["## Feature Transformation 3\n","We can also apply BERT pre-training to our feature to reduce feature number."]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Downloading: 100%|██████████| 737/737 [00:00<00:00, 184kB/s]\n","Downloading: 100%|██████████| 190/190 [00:00<00:00, 31.5kB/s]\n","Downloading: 100%|██████████| 9.90k/9.90k [00:00<00:00, 4.95MB/s]\n","Downloading: 100%|██████████| 573/573 [00:00<00:00, 287kB/s]\n","Downloading: 100%|██████████| 116/116 [00:00<00:00, 39.1kB/s]\n","Downloading: 100%|██████████| 15.7k/15.7k [00:00<00:00, 114kB/s] \n","Downloading: 100%|██████████| 349/349 [00:00<00:00, 308kB/s]\n","Downloading: 100%|██████████| 134M/134M [00:15<00:00, 8.79MB/s] \n","Downloading: 100%|██████████| 53.0/53.0 [00:00<00:00, 8.83kB/s]\n","Downloading: 100%|██████████| 112/112 [00:00<00:00, 37.3kB/s]\n","Downloading: 100%|██████████| 466k/466k [00:00<00:00, 674kB/s]  \n","Downloading: 100%|██████████| 352/352 [00:00<00:00, 177kB/s]\n","Downloading: 100%|██████████| 13.2k/13.2k [00:00<00:00, 5.78MB/s]\n","Downloading: 100%|██████████| 232k/232k [00:00<00:00, 420kB/s]  \n"]}],"source":["# Using a pre-trained BERT model\n","# It produces 384 features.\n","\n","bert_model = SentenceTransformer('all-MiniLM-L12-v1')\n","X_trainval = bert_model.encode(train_df[\"text\"])\n","feature_transform = 'BERT'"]},{"cell_type":"markdown","metadata":{},"source":["## Before Training\n","Split training set and validation set using train_test_split"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["# Split the training set into training and validation set\n","\n","y_label = train_df['target']\n","X_train, X_val, y_train, y_val = train_test_split(X_trainval, y_label, test_size=0.1, random_state=10)"]},{"cell_type":"markdown","metadata":{},"source":["## Method 1: Logistic Regression"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["# Fit the logistic regression model on the training set\n","def logistic(C, penalty, solver):\n","    logreg = LogisticRegression(C = C, max_iter = 100000, multi_class = 'ovr', penalty = penalty, solver = solver)\n","    logreg.fit(X_train, y_train)\n","    # Find the predicted values on the validation set\n","\n","    y_hat_logreg = logreg.predict(X_val)\n","    # Switch between training and testing results\n","    # y_hat_logreg = logreg.predict(X_train)\n","    \n","    return y_hat_logreg"]},{"cell_type":"markdown","metadata":{},"source":["Conducting the logistic regression:"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["current Iteration: C=0.01, penalty: none\n","current Iteration: C=0.01, penalty: l1\n","current Iteration: C=0.01, penalty: l2\n","current Iteration: C=0.1, penalty: none\n","current Iteration: C=0.1, penalty: l1\n","current Iteration: C=0.1, penalty: l2\n","current Iteration: C=1, penalty: none\n","current Iteration: C=1, penalty: l1\n","current Iteration: C=1, penalty: l2\n","current Iteration: C=10, penalty: none\n","current Iteration: C=10, penalty: l1\n","current Iteration: C=10, penalty: l2\n","current Iteration: C=100, penalty: none\n","current Iteration: C=100, penalty: l1\n","current Iteration: C=100, penalty: l2\n","current Iteration: C=1000, penalty: none\n","current Iteration: C=1000, penalty: l1\n","current Iteration: C=1000, penalty: l2\n","current Iteration: C=10000, penalty: none\n","current Iteration: C=10000, penalty: l1\n","current Iteration: C=10000, penalty: l2\n"]}],"source":["logreg_result_l1 = []\n","logreg_result_l2 = []\n","logreg_result_none = []\n","for C in [0.01, 0.1, 1, 10, 100, 1000, 10000]:\n","    for penalty in ['none','l1','l2']:\n","        if penalty == 'none':\n","            y_hat_logreg = logistic(C, penalty, solver = 'sag') # liblinear does not support no regularization\n","            print(f\"current Iteration: C={C}, penalty: {penalty}\")\n","            logreg_result_none.append(y_hat_logreg)\n","        else:\n","            y_hat_logreg = logistic(C, penalty, solver = 'liblinear')\n","            print(f\"current Iteration: C={C}, penalty: {penalty}\")\n","            if penalty == 'l1':\n","                logreg_result_l1.append(y_hat_logreg)\n","            if penalty == 'l2':\n","                logreg_result_l2.append(y_hat_logreg)"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[0.7822222222222222, 0.7822222222222222, 0.7822222222222222, 0.7822222222222222, 0.7822222222222222, 0.7822222222222222, 0.7822222222222222] [0.0, 0.7617602427921093, 0.7909774436090226, 0.7862481315396115, 0.7833827893175074, 0.7822222222222222, 0.7822222222222222] [0.7441860465116279, 0.7896341463414633, 0.7957957957957958, 0.7868852459016393, 0.7845468053491828, 0.7840236686390533, 0.7822222222222222]\n"]}],"source":["# Find Precision, recall and fscore on the validation set\n","f1_lr_none = []\n","f1_lr_l1 = []\n","f1_lr_l2 = []\n","\n","# Testing:\n","\n","for each in logreg_result_none:\n","    _, _, fscore, _ = precision_recall_fscore_support(y_val, each, average='binary')\n","    f1_lr_none.append(fscore)\n","for each in logreg_result_l1:\n","    _, _, fscore, _ = precision_recall_fscore_support(y_val, each, average='binary')\n","    f1_lr_l1.append(fscore)\n","for each in logreg_result_l2:\n","    prec, recal, fscore, _ = precision_recall_fscore_support(y_val, each, average='binary')\n","    f1_lr_l2.append(fscore)\n","\n","# Switch between training & testing results\n","\n","#Training:\n","\n","# for each in logreg_result_none:\n","#     _, _, fscore, _ = precision_recall_fscore_support(y_train, each, average='binary')\n","#     f1_lr_none.append(fscore)\n","# for each in logreg_result_l1:\n","#     _, _, fscore, _ = precision_recall_fscore_support(y_train, each, average='binary')\n","#     f1_lr_l1.append(fscore)\n","# for each in logreg_result_l2:\n","#     prec, recal, fscore, _ = precision_recall_fscore_support(y_train, each, average='binary')\n","#     f1_lr_l2.append(fscore)\n","\n","print(f1_lr_none, f1_lr_l1, f1_lr_l2)"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"data":{"text/plain":["<AxesSubplot:>"]},"execution_count":10,"metadata":{},"output_type":"execute_result"},{"data":{"image/png":"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","text/plain":["<Figure size 432x288 with 2 Axes>"]},"metadata":{"needs_background":"light"},"output_type":"display_data"}],"source":["# Confusion Matrix -> for visualization presented in paper only\n","\n","confusion_matrix_logreg = confusion_matrix(y_val, logreg_result_l2[3])\n","sns.heatmap(confusion_matrix_logreg, annot=True, fmt='d', cmap=\"Oranges\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Visualization of performance based on hyperparameters\n","\n","# plt.title(f'C vs. Training F1 score ({feature_transform})')\n","plt.title(f'C vs. Testing F1 score ({feature_transform})')\n","plt.xlabel('C')\n","plt.ylabel('F1 score')\n","C_used = ['0.01','0.1', '1', '10', '100', '1000', '10000']\n","penalties = ['none','l1','l2']\n","plt.plot(C_used, f1_lr_none, color='g', label = 'none')\n","plt.plot(C_used, f1_lr_l1, color='b', label = 'L1')\n","plt.plot(C_used, f1_lr_l2, color='r', label = 'L2')\n","plt.legend(loc='lower right')\n","plt.ylim(0,1)\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# y_hat_logreg = logistic(0.6, 'l2',  solver = 'liblinear')    # fscore: 0.793985\n","# y_hat_logreg = logistic(0.7, 'l2',  solver = 'liblinear')    # fscore: 0.795796\n","# y_hat_logreg = logistic(0.75, 'l2',  solver = 'liblinear')   # fscore: 0.796992   Best!\n","y_hat_logreg = logistic(0.8, 'l2',  solver = 'liblinear')      # fscore: 0.796992   Best!\n","# y_hat_logreg = logistic(0.85, 'l2',  solver = 'liblinear')   # fscore: 0.795796\n","# y_hat_logreg = logistic(0.9, 'l2',  solver = 'liblinear')    # fscore: 0.795796\n","# y_hat_logreg = logistic(1, 'l2',  solver = 'liblinear')      # fscore: 0.795796\n","# y_hat_logreg = logistic(1.2, 'l2',  solver = 'liblinear')    # fscore: 0.793985\n","# y_hat_logreg = logistic(1.4, 'l2',  solver = 'liblinear')    # fscore: 0.791541\n","_, _, fscore, _ = precision_recall_fscore_support(y_val, y_hat_logreg, average='binary') \n","print(fscore)\n","\n","confusion_matrix_logreg = confusion_matrix(y_val, y_hat_logreg)\n","sns.heatmap(confusion_matrix_logreg, annot=True, fmt='d', cmap=\"Oranges\")"]},{"cell_type":"markdown","metadata":{},"source":["## Method 2: SVM"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Fit the SVM model on the training set\n","def svm(C, kernel):\n","    svm = SVC(C = C, kernel = kernel)\n","    svm.fit(X_train, y_train)\n","    # Find the predicted values on the validation set\n","\n","    y_hat_svm = svm.predict(X_val)\n","    # y_hat_svm = svm.predict(X_train)\n","\n","    return y_hat_svm"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["svm_result_rbf = []\n","svm_result_linear = []\n","svm_result_poly = []\n","for C in [0.01, 0.1, 1, 10, 100, 1000]:\n","    for kernel in ['rbf','linear','poly']:\n","        y_hat_svm = svm(C, kernel)\n","        print(f\"current Iteration: C={C}, kernel: {kernel}\")\n","        if kernel == 'rbf':\n","            svm_result_rbf.append(y_hat_svm)\n","        elif kernel == 'linear':\n","            svm_result_linear.append(y_hat_svm)\n","        elif kernel == 'poly':\n","            svm_result_poly.append(y_hat_svm)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Find Precision, recall and fscore on the validation set\n","f1_svm_rbf = []\n","f1_svm_linear = []\n","f1_svm_poly = []\n","\n","\n","# for each in svm_result_rbf:\n","#     _, _, fscore, _ = precision_recall_fscore_support(y_train, each, average='binary')\n","#     f1_svm_rbf.append(fscore)\n","# for each in svm_result_linear:\n","#     _, _, fscore, _ = precision_recall_fscore_support(y_train, each, average='binary')\n","#     f1_svm_linear.append(fscore)\n","# for each in svm_result_poly:\n","#     _, _, fscore, _ = precision_recall_fscore_support(y_train, each, average='binary')\n","#     f1_svm_poly.append(fscore)\n","\n","# Switch between training & testing fscores\n","\n","for each in svm_result_rbf:\n","    _, _, fscore, _ = precision_recall_fscore_support(y_val, each, average='binary')\n","    f1_svm_rbf.append(fscore)\n","for each in svm_result_linear:\n","    _, _, fscore, _ = precision_recall_fscore_support(y_val, each, average='binary')\n","    f1_svm_linear.append(fscore)\n","for each in svm_result_poly:\n","    _, _, fscore, _ = precision_recall_fscore_support(y_val, each, average='binary')\n","    f1_svm_poly.append(fscore)\n","\n","\n","print(f1_svm_rbf, f1_svm_linear, f1_svm_poly)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Confusion Matrix\n","\n","confusion_matrix_svm = confusion_matrix(y_val, svm_result_linear[2])\n","sns.heatmap(confusion_matrix_svm, annot=True, fmt='d', cmap=\"Oranges\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Visualization of performance based on hyperparameters\n","\n","plt.title(f'C vs. Testing F1 score ({feature_transform})')\n","# plt.title(f'C vs. Training F1 score ({feature_transform})')\n","plt.xlabel('C')\n","plt.ylabel('F1 score')\n","C_used = ['0.01','0.1', '1', '10', '100', '1000']\n","plt.plot(C_used, f1_svm_rbf, color='g', label='RBF')\n","plt.plot(C_used, f1_svm_linear, color='b', label='Linear')\n","plt.plot(C_used, f1_svm_poly, color='r', label='Polynomial')\n","plt.ylim(0,1)\n","plt.legend(loc='lower right')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# y_hat_svm = svm(0.8, 'rbf')  # fscore: 0.792570\n","# y_hat_svm = svm(0.8, 'rbf')  # fscore: 0.8\n","# y_hat_svm = svm(1, 'rbf')  # fscore: 0.7987711\n","# y_hat_svm = svm(1.2, 'rbf')  # fscore: 0.804314\n","# y_hat_svm = svm(1.4, 'rbf')  # fscore: 0.807396\n","# y_hat_svm = svm(1.6, 'rbf')  # fscore: 0.811060    Best!\n","y_hat_svm = svm(1.7, 'rbf')  # fscore: 0.811060      Best!\n","# y_hat_svm = svm(1.8, 'rbf')  # fscore: 0.811060    Best!\n","# y_hat_svm = svm(2, 'rbf')  # fscore: 0.807988\n","# y_hat_svm = svm(3, 'rbf')  # fscore: 0.8\n","_, _, fscore, _ = precision_recall_fscore_support(y_val, y_hat_svm, average='binary') \n","print(fscore)\n","\n","confusion_matrix_svm = confusion_matrix(y_val, y_hat_svm)\n","sns.heatmap(confusion_matrix_svm, annot=True, fmt='d', cmap=\"Oranges\")"]},{"cell_type":"markdown","metadata":{},"source":["## Method 3: Neural Network"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Fit the neural network model on the training set\n","def neural_network(alpha, activation, nn_structure):\n","    nn = MLPClassifier(hidden_layer_sizes=nn_structure, alpha = alpha, activation = activation, max_iter=10000)\n","    nn.fit(X_train, y_train)\n","    # Find the predicted values on the validation set\n","\n","    y_hat_nn = nn.predict(X_val)\n","    # Switch between training & testing set\n","    # y_hat_nn = nn.predict(X_train)\n","\n","    return y_hat_nn"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["nn_result_logistic = []\n","nn_result_relu = []\n","nn_result_tanh = []\n","\n","nn_structure = (256, 48)\n","# Tuning neural network structure is omitted here for the sake of saving time\n","# This is the best nn_structure observed by tuning nn_structure manually\n","\n","for alpha in [0.0001,0.001,0.01,0.1, 1]:\n","    for activation in ['logistic','relu','tanh']:\n","        y_hat_nn = neural_network(alpha, activation, nn_structure)\n","        print(f\"current Iteration: alpha={alpha}, activation: {activation}\")\n","        if activation == 'logistic':\n","            nn_result_logistic.append(y_hat_nn)\n","        if activation == 'relu':\n","            nn_result_relu.append(y_hat_nn)\n","        if activation == 'tanh':\n","            nn_result_tanh.append(y_hat_nn)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["f1_nn_logistic = []\n","f1_nn_relu = []\n","f1_nn_tanh = []\n","\n","for each in nn_result_logistic:\n","    _, _, fscore, _ = precision_recall_fscore_support(y_val, each, average='binary')\n","    f1_nn_logistic.append(fscore)\n","for each in nn_result_relu:\n","    _, _, fscore, _ = precision_recall_fscore_support(y_val, each, average='binary')\n","    f1_nn_relu.append(fscore)\n","for each in nn_result_tanh:\n","    prec, recal, fscore, _ = precision_recall_fscore_support(y_val, each, average='binary')\n","    f1_nn_tanh.append(fscore)\n","\n","# Switch between training and validation\n","\n","# for each in nn_result_logistic:\n","#     _, _, fscore, _ = precision_recall_fscore_support(y_train, each, average='binary')\n","#     f1_nn_logistic.append(fscore)\n","# for each in nn_result_relu:\n","#     _, _, fscore, _ = precision_recall_fscore_support(y_train, each, average='binary')\n","#     f1_nn_relu.append(fscore)\n","# for each in nn_result_tanh:\n","#     prec, recal, fscore, _ = precision_recall_fscore_support(y_train, each, average='binary')\n","#     f1_nn_tanh.append(fscore)\n","\n","\n","print(f1_nn_logistic, f1_nn_relu, f1_nn_tanh)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Confusion Matrix\n","\n","confusion_matrix_nn = confusion_matrix(y_val, nn_result_relu[4])\n","sns.heatmap(confusion_matrix_nn, annot=True, fmt='d', cmap=\"Oranges\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Visualization of performance based on hyperparameters\n","\n","plt.title(f'lambda (L2 penalty) vs. Testing F1 score ({feature_transform})')\n","# plt.title(f'lambda (L2 penalty) vs. Training F1 score ({feature_transform})')\n","plt.xlabel('lambda')\n","plt.ylabel('F1 score')\n","C_used = ['0.0001','0.001','0.01','0.1', '1']\n","plt.plot(C_used, f1_nn_logistic, color='g', label='logistic')\n","plt.plot(C_used, f1_nn_relu, color='b', label='ReLU')\n","plt.plot(C_used, f1_nn_tanh, color='r', label='tanh')\n","plt.ylim(0,1)\n","plt.legend(loc='lower left')\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["# Method 4: Naive Bayes"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Fit the naive_bayes model on the training set\n","from sklearn.naive_bayes import BernoulliNB\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.naive_bayes import CategoricalNB\n","from sklearn.naive_bayes import ComplementNB\n","# nb = BernoulliNB(alpha=0.1)     # fscore:  0.7891737891737892\n","nb = BernoulliNB(alpha=1)         # fscore:  0.7891737891737892\n","# nb = BernoulliNB(alpha=10)      # fscore:  0.7891737891737892\n","\n","# nb = BernoulliNB(binarize=0.001)         # fscore:  0.7857142857142857\n","# nb = BernoulliNB(binarize=0.0001)         # fscore:  0.7874465049928673\n","\n","# nb = BernoulliNB(fit_prior=False)         # fscore:  0.788135593220339\n","\n","\n","# nb = GaussianNB()         # fscore:  0.7839771101573677\n","# nb = CategoricalNB()      # fscore: 0.0\n","# nb = ComplementNB()       # cannot deal with negative values\n","# nb = MultinomialNB()      # cannot deal with negative values\n","\n","nb.fit(X_train, y_train)       \n","# Find the predicted values on the validation set\n","y_hat_nb = nb.predict(X_val)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Find Precision, recall and fscore on the validation set\n","from sklearn.metrics import precision_recall_fscore_support\n","prec, recal, fscore, _ = precision_recall_fscore_support(y_val, y_hat_nb,average='binary')\n","print('prec: ', prec)\n","print('recal: ', recal)\n","print('fscore: ', fscore)\n","\n","# Confusion Matrix\n","confusion_matrix_nb = confusion_matrix(y_val, y_hat_nb)\n","sns.heatmap(confusion_matrix_nb, annot=True, fmt='d', cmap=\"Oranges\")"]},{"cell_type":"markdown","metadata":{},"source":["# Method 5: Random Forest"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Fit the random forest model on the training set\n","from sklearn.ensemble import RandomForestClassifier\n","# n_estimators = number of trees\n","# rf = RandomForestClassifier(n_estimators=10)        # fscore:  0.6885245901639344\n","# rf = RandomForestClassifier(n_estimators=50)        # fscore:  0.7576243980738362\n","# rf = RandomForestClassifier(n_estimators=90)        # fscore:  0.770440\n","rf = RandomForestClassifier(n_estimators=100)         # fscore:  0.7868338557993729\n","# rf = RandomForestClassifier(n_estimators=110)       # fscore:  0.7641509433962265\n","# rf = RandomForestClassifier(n_estimators=200)       # fscore:  0.7670364500792393\n","# rf = RandomForestClassifier(n_estimators=1000)      # fscore:  0.7733755942947703\n","rf.fit(X_train, y_train)\n","# Find the predicted values on the validation set\n","y_hat_rf = rf.predict(X_val)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Find Precision, recall and fscore on the validation set\n","from sklearn.metrics import precision_recall_fscore_support\n","prec, recal, fscore, _ = precision_recall_fscore_support(y_val, y_hat_rf,average='binary')\n","print('prec: ', prec)\n","print('recal: ', recal)\n","print('fscore: ', fscore)\n","\n","# Confusion Matrix\n","confusion_matrix_rf = confusion_matrix(y_val, y_hat_rf)\n","sns.heatmap(confusion_matrix_rf, annot=True, fmt='d', cmap=\"Oranges\")"]},{"cell_type":"markdown","metadata":{},"source":["# Method 6: Gradient Boosting"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Fit the gradient boosting model on the training set\n","from sklearn.ensemble import GradientBoostingClassifier\n","gb = GradientBoostingClassifier(learning_rate=0.1, n_estimators=100)      # fscore: 0.7914110429447853\n","# gb = GradientBoostingClassifier(learning_rate=0.1, n_estimators=99)      # fscore: 0.7914110429447853\n","# gb = GradientBoostingClassifier(learning_rate=0.08, n_estimators=125)   # fscore:  0.7870370370370371\n","# gb = GradientBoostingClassifier(learning_rate=1.1, n_estimators=90)     # fscore:  0.7478260869565216\n","gb.fit(X_train, y_train)\n","# Find the predicted values on the validation set\n","y_hat_gb = gb.predict(X_val)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Find Precision, recall and fscore on the validation set\n","from sklearn.metrics import precision_recall_fscore_support\n","prec, recal, fscore, _ = precision_recall_fscore_support(y_val, y_hat_gb,average='binary')\n","print('prec: ', prec)\n","print('recal: ', recal)\n","print('fscore: ', fscore)\n","\n","# Confusion Matrix\n","confusion_matrix_gb = confusion_matrix(y_val, y_hat_gb)\n","sns.heatmap(confusion_matrix_gb, annot=True, fmt='d', cmap=\"Oranges\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["y_hat_1 = y_hat_logreg\n","y_hat_2 = y_hat_svm\n","y_hat_3 = nn_result_relu[4]\n","_, _, fscore1, _ = precision_recall_fscore_support(y_val, y_hat_1,average='binary')\n","print('fscore: ', fscore1)\n","_, _, fscore2, _ = precision_recall_fscore_support(y_val, y_hat_2,average='binary')\n","print('fscore: ', fscore2)\n","_, _, fscore3, _ = precision_recall_fscore_support(y_val, y_hat_3,average='binary')\n","print('fscore: ', fscore3)\n","\n","y_hat_4 = []\n","for i in range(len(y_hat_logreg)):\n","    y_hat_4.append((y_hat_1[i]+(y_hat_2[i]+(y_hat_3[i]))) // 2)\n","_, _, fscore4, _ = precision_recall_fscore_support(y_val, y_hat_4,average='binary')\n","print('fscore: ', fscore4)\n","\n","y_hat_5 = []\n","for i in range(len(y_hat_logreg)):\n","    y_hat_5.append((y_hat_1[i]+(y_hat_2[i]+(y_hat_3[i]))) // 3)\n","_, _, fscore5, _ = precision_recall_fscore_support(y_val, y_hat_5,average='binary')\n","print('fscore: ', fscore5)\n","\n","# Highest fscore method!!!!!\n","y_hat_gb\n","y_hat_6 = []\n","for i in range(len(y_hat_logreg)):\n","    if (y_hat_1[i] == y_hat_3[i]) and (y_hat_3[i] == y_hat_gb[i]):\n","        y_hat_6.append(y_hat_1[i])\n","    else:\n","        y_hat_6.append(y_hat_2[i])\n","_, _, fscore6, _ = precision_recall_fscore_support(y_val, y_hat_6,average='binary')\n","print('fscore: ', fscore6)\n","\n","# Confusion Matrix\n","confusion_matrix_6 = confusion_matrix(y_val, y_hat_6)\n","sns.heatmap(confusion_matrix_6, annot=True, fmt='d', cmap=\"Oranges\")"]}],"metadata":{"interpreter":{"hash":"ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"},"kernelspec":{"display_name":"Python 3.9.7 ('base')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"}},"nbformat":4,"nbformat_minor":4}
